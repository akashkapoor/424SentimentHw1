To-Do:
✓ Generate RoC curve
✓ Perform Cross-Validation

- Classifiers
	✓ MNB
	✓ SVM
		- Cross Validate the kernels
		✓ Linear
		✓ RBF
	✓ Decision Trees
	✓ Random Forest
	✓ KNN
	✓ Logistic Regression
	✓ Gradient Boosting Classifier
	✓ Adaboost

- More Data
	✓ Clean IMDB data

- Feature Representaion
	* Terms:
		# Unigram
		# Bigram + Unigram
	* Counters:
		# Boolean
		# Frequency
		# TF-IDF

- Feature Selection
	* Information Gain
	* CPD

- Evaluation
	✓ RoC curve, AUC
	✓ Test & Train Accuracy
	✓ Precision Recall Curve & pre , recall
	✓ Confusion Matrix

Personal Notes:
Steps:
1. Use given script to build vocabulary and bag of words represetation for each review.
	1.1 Feel free to extend as necessary.
2. Use some sort of feature selector to reduce the number of features.
3. Try different classifiers
4. Evaluate each of the classifiers in the following metrics:
	* Receiver Operating Characteristics curves on test data set.
	* Number of false positives & False negatives.

	* Most important features
	* What do these features tell us about the problem
	* What is worse false-positive or false-negative?
	* Reviews that worked well on all approaches, reviews that differed.

Extensions:
- Data:
	* IMDb movies data set w/ 50k 
	* Some other reviews ??
- Representation:
	* Bag-of-words binary vector
	* Bag-of-words Frequency vector
	* Bi-Grams
	* TF-IDF ??
- Feature selection:
	* Adjectives
	* ???
- Other extensions:
	* Cross Validation 
- Evaluation:
	* RoC
		- Good if balanced dataset.
	* Precision Recall Curve

Links:
- Report: http://sentiment.christopherpotts.net/classifiers.html
- https://www.youtube.com/watch?v=6dbrR-WymjI [Feature selection through CV]
- http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/



SVM: 
- http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html
- http://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html

http://blog.datumbox.com/machine-learning-tutorial-the-max-entropy-text-classifier/